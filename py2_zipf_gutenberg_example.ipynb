{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOWTO install and create a conda environment with Python 2 with the required packages for Linux in a bash terminal.\n",
    "# 1. Download and install miniconda2 from\n",
    "#    https://conda.io/en/latest/miniconda.html\n",
    "# 2. Activate the base environment for conda\n",
    "# 3. Create a new environment\n",
    "#    $ conda create --name py2_zipf_music python=3\n",
    "# 4. Activate the new environment\n",
    "#    $ conda activate py2_zipf_music\n",
    "# 5. Install the required Python packages\n",
    "#    $ conda install -c anaconda numpy\n",
    "#    $ conda install -c conda-forge matplotlib\n",
    "#    $ conda install -c anaconda jupyter \n",
    "#    $ conda install -c anaconda nltk\n",
    "# 6. Run Jupyter notebook\n",
    "#    $ jupyter notebook\n",
    "# 7. and open py2_zipf_gutenberg_example.ipynb\n",
    "# 8. The code automatically downloads the text example from Project Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (13,9)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import urllib2\n",
    "import urllib\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import urllib\n",
    "import string\n",
    "import nltk as nl\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The n-gramCA\n",
    "\n",
    "def seq_ngram( seq , n , t ):\n",
    "    #return \"-\".join( seq[ t : t + n ] )\n",
    "    return \"%\".join( seq[ t : t + n ] )\n",
    "\n",
    "def it_ngram( seq , n ):\n",
    "    assert n > 1\n",
    "    for t in xrange( len( seq ) - n + 1 ):\n",
    "        yield seq_ngram( seq , n , t )\n",
    "    \n",
    "def n_gramCA( _seq , n_max = 20 , safety = None , verbose = 0 ):\n",
    "    #N = len( _seq )\n",
    "    #w_f = defaultdict( int )\n",
    "    #for w in _seq:\n",
    "    #    w_f[ w ] += 1\n",
    "    if safety is None:\n",
    "        safety = len( _seq ) * n_max\n",
    "    seq = list( _seq )\n",
    "    large_enough = False\n",
    "    n = n_max\n",
    "    while n > 1 and safety > 0:\n",
    "        if verbose > 0:\n",
    "            print '### n',n\n",
    "        ngram_idxs  = defaultdict( list )\n",
    "        #ngram_f     = defaultdict( int )\n",
    "        for t , ngram in enumerate( it_ngram( seq , n ) ):\n",
    "            #ngram_f[ ngram ] += 1\n",
    "            list_t = ngram_idxs[ ngram ]\n",
    "            if len( list_t ) > 0:\n",
    "                if t - list_t[ -1 ] < n:\n",
    "                    continue\n",
    "            ngram_idxs[ ngram ].append( t )\n",
    "        z_ngrams = sorted( [ ( len( list_t ) , ngram , list_t ) for ngram , list_t in ngram_idxs.items() if len( list_t ) > 1 ] , reverse = True )\n",
    "        #z_ngrams = sorted( [ ( log_score( ngram , len( list_t ) , w_f , N ) , ngram , list_t ) for ngram , list_t in ngram_idxs.items() if len( list_t ) > 1 ] , reverse = True )\n",
    "        if len( z_ngrams ) == 0:\n",
    "            if verbose > 0:\n",
    "                print '### large_enough n',n\n",
    "            large_enough = True\n",
    "            n -= 1\n",
    "        else:\n",
    "            assert large_enough , \"ERROR : n_max is too small...\"\n",
    "            dummy , ngram , list_t = z_ngrams[ 0 ]\n",
    "            for t in sorted( list_t ):\n",
    "                seq = seq[ : t ] + [ ngram ] + [ \"\" ] * ( n - 1 ) + seq[ t + n : ]\n",
    "            seq = [ s for s in seq if s != \"\" ]\n",
    "        safety -= 1\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the .txt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donwload Metamorphosis .txt file.\n",
    "url = \"http://www.gutenberg.org/cache/epub/5200/pg5200.txt\"\n",
    "response = urllib.urlopen(url)\n",
    "raw = response.read().decode('utf8')\n",
    "raw[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curate the text and convert it into a sequence of lower-case words without punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut the non-original parts of the text.\n",
    "cut=raw[871:871+121115]\n",
    "print 'BEGINNING...'\n",
    "print cut[:20]\n",
    "print 'END...'\n",
    "print cut[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_ascii=unicodedata.normalize('NFKD',cut).encode('ascii','ignore') # Transforma UTF8 a ascii\n",
    "cut_ascii[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nl.word_tokenize(cut_ascii)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nl.Text(tokens)\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [ w.lower() for w in words ]\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation but keep contractions like \"can't\" together.\n",
    "word_sequence = [ sw for sw in [ str( w ).strip( string.punctuation ) for w in words ] if len( sw ) > 0 ]\n",
    "word_sequence[:10]\n",
    "#sorted( set( stripped ) )[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the compressed word sequence\n",
    "compressed_word_sequence = n_gramCA( word_sequence , n_max = 20 , safety = None , verbose = 0 )\n",
    "compressed_word_sequence[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_word_sequence[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute refragmented word sequence\n",
    "eta = 0.4\n",
    "tmptext = ' '.join(compressed_word_sequence)\n",
    "new_tmptext = []\n",
    "for i in xrange( len( tmptext ) ):\n",
    "    s = tmptext[ i ] \n",
    "    if s == '%' and np.random.random() < eta:\n",
    "        new_tmptext.append( ' ' )\n",
    "    else:\n",
    "        new_tmptext.append( s )\n",
    "refragmented_word_sequence = ''.join(new_tmptext).split()       \n",
    "refragmented_word_sequence[:10]        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the rank-frequency plot of the original word sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank-frequency distribution of original word frequency.\n",
    "f_w = defaultdict(float)\n",
    "for w in word_sequence:\n",
    "    f_w[w] += 1.0\n",
    "word_ranks = []\n",
    "word_frequencies = []\n",
    "for rr,(f,w) in enumerate(sorted([(f,w) for (w,f) in f_w.items()],reverse=True)):\n",
    "    r=rr+1\n",
    "    #print r,f,w\n",
    "    word_ranks.append(r)\n",
    "    word_frequencies.append(f)\n",
    "    \n",
    "# Rank-frequency distribution of the compressed word frequency.    \n",
    "f_w = defaultdict(float)\n",
    "for w in compressed_word_sequence:\n",
    "    f_w[w] += 1.0\n",
    "compressed_word_ranks = []\n",
    "compressed_word_frequencies = []\n",
    "for rr,(f,w) in enumerate(sorted([(f,w) for (w,f) in f_w.items()],reverse=True)):\n",
    "    r=rr+1\n",
    "    #print r,f,w\n",
    "    compressed_word_ranks.append(r)\n",
    "    compressed_word_frequencies.append(f)    \n",
    "    \n",
    "# Rank-frequency distribution of the compressed word frequency.    \n",
    "f_w = defaultdict(float)\n",
    "for w in refragmented_word_sequence:\n",
    "    f_w[w] += 1.0\n",
    "refragmented_word_ranks = []\n",
    "refragmented_word_frequencies = []\n",
    "for rr,(f,w) in enumerate(sorted([(f,w) for (w,f) in f_w.items()],reverse=True)):\n",
    "    r=rr+1\n",
    "    #print r,f,w\n",
    "    refragmented_word_ranks.append(r)\n",
    "    refragmented_word_frequencies.append(f)       \n",
    "    \n",
    "# Plot curves in log-log   \n",
    "plt.title(\"Metamorphosis\")\n",
    "plt.xlabel(\"r\")\n",
    "plt.ylabel(\"f\");\n",
    "plt.loglog()\n",
    "\n",
    "zipf_law_ranks = range(10,2000)\n",
    "zipf_law_frequencies = [3500.0/r for r in zipf_law_ranks]\n",
    "\n",
    "plt.plot(word_ranks, word_frequencies,label=\"words\")\n",
    "plt.plot(compressed_word_ranks, compressed_word_frequencies,label=\"compressed words\")\n",
    "plt.plot(refragmented_word_ranks, refragmented_word_frequencies,label=\"refragmented words\")\n",
    "plt.plot(zipf_law_ranks, zipf_law_frequencies,label=\"Zipf's law\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
